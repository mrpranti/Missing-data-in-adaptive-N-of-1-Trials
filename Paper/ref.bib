@article{Shrestha2021,
   abstract = {N-of-1 trials, which are randomized, double-blinded, controlled, multiperiod, crossover trials on a single subject, have been applied to determine the heterogeneity of the individual's treatment effect in precision medicine settings. An aggregated N-of-1 design, which can estimate the population effect from these individual trials, is a pragmatic alternative when a randomized controlled trial (RCT) is infeasible. We propose a Bayesian adaptive design for both the individual and aggregated N-of-1 trials using a multiarmed bandit framework that is estimated via efficient Markov chain Monte Carlo. A Bayesian hierarchical structure is used to jointly model the individual and population treatment effects. Our proposed adaptive trial design is based on Thompson sampling, which randomly allocates individuals to treatments based on the Bayesian posterior probability of each treatment being optimal. While we use a subject-specific treatment effect and Bayesian posterior probability estimates to determine an individual's treatment allocation, our hierarchical model facilitates these individual estimates to borrow strength from the population estimates via shrinkage to the population mean. We present the design's operating characteristics and performance via a simulation study motivated by a recently completed N-of-1 clinical trial. We demonstrate that from a patient-centered perspective, subjects are likely to benefit from our adaptive design, in particular, for those individuals that deviate from the overall population effect.},
   author = {Sama Shrestha and Sonia Jain},
   doi = {10.1002/sim.8873},
   issn = {10970258},
   issue = {7},
   journal = {Statistics in Medicine},
   keywords = {Bayesian adaptive design,Markov chain Monte Carlo,N-of-1 trials,Thompson sampling,multiarmed bandit,precision medicine},
   month = {3},
   pages = {1825-1844},
   pmid = {33462851},
   publisher = {John Wiley and Sons Ltd},
   title = {A Bayesian-bandit adaptive design for N-of-1 clinical trials},
   volume = {40},
   year = {2021},
}

@article{Senarathne2020,
   abstract = {This article proposes a novel adaptive design algorithm that can be used to find optimal treatment allocations in N-of-1 clinical trials. This new methodology uses two Laplace approximations to provide a computationally efficient estimate of population and individual random effects within a repeated measures, adaptive design framework. Given the efficiency of this approach, it is also adopted for treatment selection to target the collection of data for the precise estimation of treatment effects. To evaluate this approach, we consider both a simulated and motivating N-of-1 clinical trial from the literature. For each trial, our methods were compared with the multiarmed bandit approach and a randomized N-of-1 trial design in terms of identifying the best treatment for each patient and the information gained about the model parameters. The results show that our new approach selects designs that are highly efficient in achieving each of these objectives. As such, we propose our Laplace-based algorithm as an efficient approach for designing adaptive N-of-1 trials.},
   author = {Siththara Gedara J. Senarathne and Antony M. Overstall and James M. McGree},
   doi = {10.1002/sim.8737},
   issn = {10970258},
   issue = {29},
   journal = {Statistics in Medicine},
   keywords = {Laplace approximation,mixed effects model,multiarmed bandit design,parameter estimation,placebo,random effect},
   month = {12},
   pages = {4499-4518},
   pmid = {32969513},
   publisher = {John Wiley and Sons Ltd},
   title = {Bayesian adaptive N-of-1 trials for estimating population and individual treatment effects},
   volume = {39},
   year = {2020},
}
@article{Bouneffouf2020,
   abstract = {We consider a novel variant of the contextual bandit problem (i.e., the multi-armed bandit with side-information, or context, available to a decision-maker) where the reward associated with each context-based decision may not always be observed("missing rewards"). This new problem is motivated by certain online settings including clinical trial and ad recommendation applications. In order to address the missing rewards setting, we propose to combine the standard contextual bandit approach with an unsupervised learning mechanism such as clustering. Unlike standard contextual bandit methods, by leveraging clustering to estimate missing reward, we are able to learn from each incoming event, even those with missing rewards. Promising empirical results are obtained on several real-life datasets.},
   author = {Djallel Bouneffouf and Sohini Upadhyay and Yasaman Khazaeni},
   journal={arXiv preprint arXiv:2007.06368},
   month = {7},
   title = {Contextual Bandit with Missing Rewards},
   url = {http://arxiv.org/abs/2007.06368},
   year = {2020},
}
@misc{Kim,
   abstract = {A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing.},
   author = {Wonyoung Kim and Gi-Soo Kim and Myunghee Cho Paik},
   note = {While Kim and Paik [2019] uses Lasso estimator with pseudo-rewards aggregated over allarms, we use ridge regression estimator with pseudo-rewards in (1) which are deÔ¨Åned separately foreach i = 1, . . . ,N.},
   title = {Doubly Robust Thompson Sampling with Linear Payoffs},
}
@inproceedings{Yamaguchi2020,
   abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, real world learning tasks often involve complex patterns of missing data, and model-based reinforcement learning cannot handle missing data directly. To overcome this problem, in this paper, we focus on M-PGPE(GP) proposed by Mori et al. as a model-based reinforcement learning, and propose an extension of M-PGPE(GP) to handle missing data, which we call MM-PGPE(GP). The performance of the proposed MM-PGPE(GP) is assessed in two experiments with mountain car task. These experiments highlight the MM-PGPE(GP) produces higher average return and outperforms the conventional M-PGPE(GP) with simple linear interpolation.},
   author = {Nobuhiko Yamaguchi and Osamu Fukuda and Hiroshi Okumura},
   doi = {10.1109/CANDARW51189.2020.00042},
   isbn = {9781728199191},
   booktitle = {Proceedings - 2020 8th International Symposium on Computing and Networking Workshops, CANDARW 2020},
   keywords = {PGPE,missing data,model-based reinforcement learning,reinforcement learning},
   month = {11},
   pages = {168-171},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Model-based reinforcement learning with missing data},
   year = {2020},
}
@article{Chen2022,
   abstract = {When comparing the performance of multi-armed bandit algorithms, the potential impact of missing data is often overlooked. In practice, it also affects their implementation where the simplest approach to overcome this is to continue to sample according to the original bandit algorithm, ignoring missing outcomes. We investigate the impact on performance of this approach to deal with missing data for several bandit algorithms through an extensive simulation study assuming the rewards are missing at random. We focus on two-armed bandit algorithms with binary outcomes in the context of patient allocation for clinical trials with relatively small sample sizes. However, our results apply to other applications of bandit algorithms where missing data is expected to occur. We assess the resulting operating characteristics, including the expected reward. Different probabilities of missingness in both arms are considered. The key finding of our work is that when using the simplest strategy of ignoring missing data, the impact on the expected performance of multi-armed bandit strategies varies according to the way these strategies balance the exploration-exploitation tradeoff. Algorithms that are geared towards exploration continue to assign samples to the arm with more missing responses (which being perceived as the arm with less observed information is deemed more appealing by the algorithm than it would otherwise be). In contrast, algorithms that are geared towards exploitation would rapidly assign a high value to samples from the arms with a current high mean irrespective of the level observations per arm. Furthermore, for algorithms focusing more on exploration, we illustrate that the problem of missing responses can be alleviated using a simple mean imputation approach.},
   author = {Xijin Chen and Kim May Lee and Sofia S. Villar and David S. Robertson},
   doi = {10.1371/journal.pone.0274272},
   issn = {19326203},
   issue = {9 September},
   journal = {PLoS ONE},
   month = {9},
   note = {Method: using a simple mean imputation approach.},
   pmid = {36094920},
   publisher = {Public Library of Science},
   title = {Some performance considerations when using multi-armed bandit algorithms in the presence of missing data},
   volume = {17},
   year = {2022},
}
@article{Wei1990,
   author = {Greg C. G. Wei and Martin A. Tanner},
   doi = {10.1080/01621459.1990.10474930},
   issn = {0162-1459},
   issue = {411},
   journal = {Journal of the American Statistical Association},
   month = {9},
   pages = {699-704},
   title = {A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithms},
   volume = {85},
   year = {1990},
}


